loading data...
ball1-180.txt
ball10-370.txt
ball2-200.txt
ball3-230.txt
ball4-330.txt
ball5-360.txt
ball6-300.txt
ball7-320.txt
ball8-320.txt
ball9-430.txt
bane1-265.txt
bane10-400.txt
bane2-260.txt
bane3-250.txt
bane4-350.txt
bane5-380.txt
bane6-320.txt
bane7-250.txt
bane8-350.txt
bane9-380.txt
dance1-180.txt
dance10-250.txt
dance2-300.txt
dance3-250.txt
dance4-300.txt
dance5-280.txt
dance6-250.txt
dance7-300.txt
dance8-200.txt
dance9-250.txt
date1-290.txt
date10-280.txt
date2-200.txt
date3-240.txt
date4-300.txt
date5-220.txt
date6-280.txt
date7-380.txt
date8-300.txt
date9-300.txt
game1-200.txt
game10-290.txt
game2-180.txt
game3-180.txt
game4-330.txt
game5-380.txt
game6-370.txt
game7-320.txt
game8-250.txt
game9-420.txt
gap1-240.txt
gap10-340.txt
gap2-200.txt
gap3-240.txt
gap4-350.txt
gap5-300.txt
gap6-300.txt
gap7-280.txt
gap8-300.txt
gap9-280.txt
dataset done loading
number of training examples: 60
dataset shape: torch.Size([60, 16, 15])
0 loss: 0.01839576562245687
25 loss: 0.01828994552294413
50 loss: 0.018280468384424844
75 loss: 0.018269209067026775
100 loss: 0.018255076805750527
125 loss: 0.01823638677597046
150 loss: 0.018210341533025105
175 loss: 0.018171987930933633
200 loss: 0.018112154801686604
225 loss: 0.0180130660533905
250 loss: 0.01783971587816874
275 loss: 0.0175266961256663
300 loss: 0.016975841919581094
325 loss: 0.016110743085543316
350 loss: 0.014951136708259583
375 loss: 0.013555587331453959
400 loss: 0.012137859066327413
425 loss: 0.01093585193157196
450 loss: 0.009954477349917093
475 loss: 0.009115524093310038
Finished Training
tensor([[ 2.0503, -1.5990, -0.0111],
        [ 2.6326, -1.9409, -0.2689],
        [ 2.8938, -2.1285, -0.3489],
        [ 2.6148, -1.9018, -0.3011],
        [ 2.6733, -1.9712, -0.2807],
        [ 2.5397, -1.9071, -0.2014],
        [ 2.7845, -2.0464, -0.3198],
        [ 2.7603, -2.0077, -0.3288],
        [ 2.7305, -1.9884, -0.3235],
        [ 2.7112, -1.9894, -0.2937],
        [ 2.0469, -1.3459, -0.2429],
        [ 0.7918, -0.4635,  0.2082],
        [ 0.0643,  0.3539,  0.2015],
        [ 2.7256, -1.9959, -0.3132],
        [ 2.1827, -1.4678, -0.2328],
        [ 1.7059, -1.2716,  0.0298],
        [ 1.3416, -1.0288,  0.1799],
        [ 1.6489, -1.3559,  0.1655],
        [ 2.1724, -1.5585, -0.1706],
        [ 0.3462, -0.3674,  0.5533],
        [-1.5415,  1.2419,  1.0117],
        [-2.6385,  2.1225,  1.2623],
        [-0.1167,  0.0608,  0.6343],
        [-0.9250,  0.9360,  0.6516],
        [-1.8436,  1.4835,  1.0738],
        [-1.9895,  1.7003,  1.0178],
        [-2.7065,  2.1482,  1.3143],
        [-1.5279,  1.3548,  0.8824],
        [-1.9888,  1.7086,  1.0000],
        [-2.6407,  2.1355,  1.2513],
        [-1.0147,  1.0377,  0.6734],
        [-1.4088,  1.0828,  1.0087],
        [-1.5224,  1.3839,  0.8334],
        [-2.2242,  1.8837,  1.0868],
        [-2.1576,  1.7557,  1.1517],
        [-1.0039,  0.9236,  0.7641],
        [-0.6311,  0.6626,  0.6361],
        [-2.5856,  2.0672,  1.2693],
        [-0.8665,  0.6664,  0.8635],
        [-2.1427,  1.7221,  1.1587],
        [-0.3849,  0.3555,  0.6487],
        [-0.3908,  0.2261,  0.7672],
        [-0.9265,  0.8168,  0.7677],
        [-0.5749,  0.1636,  1.0095],
        [ 0.0396, -0.0700,  0.6221],
        [-0.0954,  0.1362,  0.5681],
        [-0.4956,  0.4570,  0.6636],
        [ 0.0751, -0.1212,  0.6176],
        [-0.3508,  0.1593,  0.7798],
        [ 0.1316, -0.1927,  0.6224],
        [ 0.1760, -0.1930,  0.5600],
        [-1.7379,  1.2890,  1.1651],
        [-1.1545,  0.9562,  0.8842],
        [-1.7854,  1.3541,  1.1507],
        [-1.5892,  1.2043,  1.0873],
        [-1.1610,  0.7773,  1.0345],
        [-1.2791,  0.7761,  1.1372],
        [-1.6459,  1.1329,  1.1923],
        [-1.6607,  1.1834,  1.1745],
        [-0.9070,  0.4683,  1.0464]], grad_fn=<AddmmBackward0>)
