loading data...
dataset done loading
number of training examples: 180
dataset shape: torch.Size([180, 16, 15])
0 loss: 0.0062220805221133765
25 loss: 0.00611168278588189
50 loss: 0.006109944317075941
75 loss: 0.006108366780810886
100 loss: 0.006106913089752197
125 loss: 0.0061055547661251494
150 loss: 0.006104264656702677
175 loss: 0.006103020906448364
200 loss: 0.0061018049716949465
225 loss: 0.0061005963219536675
250 loss: 0.006099379062652588
275 loss: 0.006098132663302952
300 loss: 0.006096839242511325
325 loss: 0.006095478269788954
350 loss: 0.00609402789009942
375 loss: 0.006092456314298842
400 loss: 0.006090732415517172
425 loss: 0.006088817119598388
450 loss: 0.006086655457814534
475 loss: 0.006084181865056356
Finished Training
tensor([[-0.1014, -0.0929, -0.1080],
        [-0.1004, -0.0840, -0.0967],
        [-0.1022, -0.0904, -0.0909],
        [-0.1029, -0.0976, -0.1039],
        [-0.1006, -0.0835, -0.1075],
        [-0.0975, -0.0756, -0.1028],
        [-0.0988, -0.0807, -0.1064],
        [-0.0924, -0.0785, -0.1045],
        [-0.1014, -0.0782, -0.1070],
        [-0.0976, -0.0818, -0.0945],
        [-0.0993, -0.1357, -0.1182],
        [-0.1058, -0.1376, -0.1093],
        [-0.1042, -0.1590, -0.1084],
        [-0.0987, -0.1530, -0.1044],
        [-0.1005, -0.1256, -0.1197],
        [-0.1086, -0.1218, -0.1208],
        [-0.1019, -0.1304, -0.1165],
        [-0.1032, -0.1263, -0.1096],
        [-0.1029, -0.1305, -0.1127],
        [-0.1039, -0.1363, -0.1162],
        [-0.1086, -0.1537, -0.1093],
        [-0.0996, -0.1475, -0.1219],
        [-0.1058, -0.1421, -0.1169],
        [-0.1085, -0.1463, -0.1156],
        [-0.1013, -0.1428, -0.1231],
        [-0.1059, -0.1438, -0.1247],
        [-0.1029, -0.1466, -0.1161],
        [-0.1055, -0.1485, -0.1188],
        [-0.0999, -0.1428, -0.1156],
        [-0.0968, -0.1506, -0.1149],
        [-0.0936, -0.1301, -0.0973],
        [-0.0989, -0.1179, -0.1118],
        [-0.0964, -0.1404, -0.1125],
        [-0.1004, -0.1218, -0.1065],
        [-0.1039, -0.1103, -0.1106],
        [-0.1009, -0.1052, -0.1151],
        [-0.1025, -0.1250, -0.1111],
        [-0.0979, -0.1225, -0.1046],
        [-0.0978, -0.1176, -0.1170],
        [-0.1019, -0.1076, -0.1146],
        [-0.0915, -0.1205, -0.0978],
        [-0.0981, -0.1037, -0.1094],
        [-0.0940, -0.1197, -0.0994],
        [-0.0914, -0.1201, -0.1004],
        [-0.0947, -0.0926, -0.1072],
        [-0.1064, -0.1014, -0.1033],
        [-0.1099, -0.1347, -0.0971],
        [-0.1009, -0.1042, -0.1118],
        [-0.1061, -0.1006, -0.1048],
        [-0.0989, -0.0990, -0.1102],
        [-0.0986, -0.1280, -0.0911],
        [-0.1036, -0.1061, -0.1074],
        [-0.0999, -0.1339, -0.0892],
        [-0.0937, -0.1411, -0.0884],
        [-0.0984, -0.1233, -0.0860],
        [-0.0986, -0.1130, -0.1040],
        [-0.0962, -0.1155, -0.1056],
        [-0.0979, -0.1165, -0.1026],
        [-0.0980, -0.1164, -0.0963],
        [-0.0996, -0.1029, -0.1139],
        [-0.0997, -0.1250, -0.0935],
        [-0.0988, -0.1041, -0.1093],
        [-0.0999, -0.0980, -0.1070],
        [-0.0978, -0.1255, -0.0930],
        [-0.0974, -0.1072, -0.0971],
        [-0.1037, -0.1040, -0.1070],
        [-0.1014, -0.1019, -0.1051],
        [-0.0975, -0.0992, -0.1089],
        [-0.1008, -0.1184, -0.0955],
        [-0.1006, -0.0989, -0.1128],
        [-0.0948, -0.1428, -0.1136],
        [-0.1005, -0.1293, -0.1225],
        [-0.0998, -0.1416, -0.1235],
        [-0.0988, -0.1389, -0.1202],
        [-0.1017, -0.1356, -0.1202],
        [-0.1011, -0.1317, -0.1110],
        [-0.1045, -0.1348, -0.1179],
        [-0.0983, -0.1277, -0.1240],
        [-0.1011, -0.1330, -0.1140],
        [-0.0964, -0.1254, -0.1195],
        [-0.0995, -0.1182, -0.0908],
        [-0.0974, -0.0959, -0.1043],
        [-0.0971, -0.1185, -0.0916],
        [-0.0960, -0.1248, -0.0900],
        [-0.0973, -0.1014, -0.1000],
        [-0.0979, -0.0957, -0.1030],
        [-0.0956, -0.0938, -0.1048],
        [-0.0958, -0.0960, -0.0996],
        [-0.0945, -0.0982, -0.0981],
        [-0.1041, -0.0819, -0.1100],
        [-0.0985, -0.1056, -0.0994],
        [-0.1043, -0.0926, -0.1097],
        [-0.1000, -0.1070, -0.1005],
        [-0.1028, -0.1108, -0.1046],
        [-0.1045, -0.0835, -0.1027],
        [-0.0984, -0.0909, -0.1075],
        [-0.1033, -0.0794, -0.1063],
        [-0.1024, -0.0869, -0.1095],
        [-0.0991, -0.0844, -0.1107],
        [-0.1004, -0.0951, -0.1026],
        [-0.0990, -0.1177, -0.1018],
        [-0.1014, -0.1029, -0.1087],
        [-0.1112, -0.1039, -0.1148],
        [-0.1074, -0.1078, -0.1075],
        [-0.1066, -0.1047, -0.1073],
        [-0.1095, -0.0981, -0.1060],
        [-0.1036, -0.1041, -0.1094],
        [-0.1093, -0.1042, -0.1120],
        [-0.1039, -0.1024, -0.1099],
        [-0.1062, -0.1035, -0.1091],
        [-0.1010, -0.0899, -0.1133],
        [-0.1036, -0.0827, -0.1080],
        [-0.1012, -0.0967, -0.1119],
        [-0.0982, -0.0906, -0.1034],
        [-0.1078, -0.0952, -0.1127],
        [-0.0980, -0.0847, -0.1126],
        [-0.1002, -0.0883, -0.1084],
        [-0.1083, -0.0763, -0.1130],
        [-0.1032, -0.0757, -0.1161],
        [-0.1099, -0.0706, -0.1151],
        [-0.1024, -0.1532, -0.0992],
        [-0.1019, -0.1317, -0.1166],
        [-0.1009, -0.1532, -0.1012],
        [-0.0809, -0.1549, -0.0858],
        [-0.1022, -0.1311, -0.1164],
        [-0.1006, -0.1260, -0.1173],
        [-0.1010, -0.1327, -0.1114],
        [-0.1016, -0.1293, -0.1148],
        [-0.0927, -0.1663, -0.0990],
        [-0.1010, -0.1255, -0.1185],
        [-0.1042, -0.1052, -0.1069],
        [-0.1019, -0.0834, -0.1131],
        [-0.1073, -0.1106, -0.0992],
        [-0.1024, -0.1014, -0.1112],
        [-0.1039, -0.0869, -0.1132],
        [-0.0943, -0.0861, -0.1131],
        [-0.1052, -0.0903, -0.1117],
        [-0.1049, -0.0841, -0.1123],
        [-0.1013, -0.0893, -0.1064],
        [-0.1020, -0.0827, -0.1115],
        [-0.1092, -0.0987, -0.1260],
        [-0.1050, -0.1252, -0.1072],
        [-0.1105, -0.1235, -0.1106],
        [-0.1087, -0.1178, -0.1183],
        [-0.1050, -0.1291, -0.1045],
        [-0.0970, -0.1522, -0.1104],
        [-0.1064, -0.1226, -0.1057],
        [-0.1022, -0.1190, -0.1124],
        [-0.1133, -0.1060, -0.1038],
        [-0.1030, -0.1166, -0.1147],
        [-0.1107, -0.1432, -0.0951],
        [-0.1042, -0.1061, -0.1001],
        [-0.1005, -0.1161, -0.0911],
        [-0.1027, -0.1220, -0.0935],
        [-0.1087, -0.0944, -0.1043],
        [-0.1073, -0.1091, -0.0985],
        [-0.1088, -0.1074, -0.0987],
        [-0.0936, -0.1440, -0.0961],
        [-0.1030, -0.1033, -0.1039],
        [-0.1024, -0.1024, -0.1024],
        [-0.0973, -0.1339, -0.0852],
        [-0.0960, -0.1193, -0.0919],
        [-0.0964, -0.1372, -0.0833],
        [-0.1002, -0.1414, -0.0761],
        [-0.1039, -0.1266, -0.1031],
        [-0.0965, -0.1177, -0.0918],
        [-0.0952, -0.0913, -0.1061],
        [-0.1030, -0.1183, -0.0985],
        [-0.1014, -0.1149, -0.0971],
        [-0.0979, -0.1266, -0.0960],
        [-0.1005, -0.1090, -0.1120],
        [-0.1087, -0.1424, -0.0967],
        [-0.0998, -0.1154, -0.0942],
        [-0.0962, -0.1143, -0.0995],
        [-0.0970, -0.1025, -0.0990],
        [-0.1027, -0.1000, -0.1017],
        [-0.1007, -0.0962, -0.1000],
        [-0.0983, -0.0927, -0.1056],
        [-0.1001, -0.0924, -0.1013],
        [-0.0974, -0.0984, -0.0989]], grad_fn=<AddmmBackward0>)
Accuracy of the network on the test samples: 51 %
