{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5f2de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "\"\"\"Time Delay Neural Network as mentioned in the 1989 paper by Waibel et al. (Hinton) and the 2015 paper by Peddinti et al. (Povey)\"\"\"\n",
    "\n",
    "class TDNN(nn.Module):\n",
    "    def __init__(self, context, input_dim, output_dim, full_context = True):\n",
    "        \"\"\"\n",
    "        Definition of context is the same as the way it's defined in the Peddinti paper. It's a list of integers, eg: [-2,2]\n",
    "        By deault, full context is chosen, which means: [-2,2] will be expanded to [-2,-1,0,1,2] i.e. range(-2,3)\n",
    "        \"\"\"\n",
    "        super(TDNN,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.check_valid_context(context)\n",
    "        self.kernel_width, context = self.get_kernel_width(context,full_context)\n",
    "        self.register_buffer('context',torch.LongTensor(context))\n",
    "        self.full_context = full_context\n",
    "        stdv = 1./math.sqrt(input_dim)\n",
    "        self.kernel = nn.Parameter(torch.Tensor(output_dim, input_dim, self.kernel_width).normal_(0,stdv))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim).normal_(0,stdv))\n",
    "        # self.cuda_flag = False\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        x is one batch of data\n",
    "        x.size(): [batch_size, sequence_length, input_dim]\n",
    "        sequence length is the length of the input spectral data (number of frames) or if already passed through the convolutional network, it's the number of learned features\n",
    "        output size: [batch_size, output_dim, len(valid_steps)]\n",
    "        \"\"\"\n",
    "        # Check if parameters are cuda type and change context\n",
    "        # if type(self.bias.data) == torch.cuda.FloatTensor and self.cuda_flag == False:\n",
    "        #     self.context = self.context.cuda()\n",
    "        #     self.cuda_flag = True\n",
    "        conv_out = self.special_convolution(x, self.kernel, self.context, self.bias)\n",
    "        return F.relu(conv_out)\n",
    "\n",
    "    def special_convolution(self, x, kernel, context, bias):\n",
    "        \"\"\"\n",
    "        This function performs the weight multiplication given an arbitrary context. Cannot directly use convolution because in case of only particular frames of context,\n",
    "        one needs to select only those frames and perform a convolution across all batch items and all output dimensions of the kernel.\n",
    "        \"\"\"\n",
    "        input_size = x.size()\n",
    "        assert len(input_size) == 3, 'Input tensor dimensionality is incorrect. Should be a 3D tensor'\n",
    "        [batch_size, input_sequence_length, input_dim] = input_size\n",
    "        x = x.transpose(1,2).contiguous()\n",
    "\n",
    "        # Allocate memory for output\n",
    "        valid_steps = self.get_valid_steps(self.context, input_sequence_length)\n",
    "        xs = Variable(self.bias.data.new(batch_size, kernel.size()[0], len(valid_steps)))\n",
    "\n",
    "        # Perform the convolution with relevant input frames\n",
    "        for c, i in enumerate(valid_steps):\n",
    "            features = torch.index_select(x, 2, context+i)\n",
    "            xs[:,:,c] = F.conv1d(features, kernel, bias = bias)[:,:,0]\n",
    "        return xs\n",
    "\n",
    "    @staticmethod\n",
    "    def check_valid_context(context):\n",
    "        # here context is still a list\n",
    "        assert context[0] <= context[-1], 'Input tensor dimensionality is incorrect. Should be a 3D tensor'\n",
    "\n",
    "    @staticmethod\n",
    "    def get_kernel_width(context, full_context):\n",
    "        if full_context:\n",
    "            context = range(context[0],context[-1]+1)\n",
    "        return len(context), context\n",
    "\n",
    "    @staticmethod\n",
    "    def get_valid_steps(context, input_sequence_length):\n",
    "        start = 0 if context[0] >= 0 else -1*context[0]\n",
    "        end = input_sequence_length if context[-1] <= 0 else input_sequence_length - context[-1]\n",
    "        return range(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9384ca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 16, 15])\n",
      "torch.Size([50, 8, 15])\n",
      "torch.Size([50, 3, 15])\n",
      "torch.Size([50, 50, 15])\n",
      "tensor(0.3142, grad_fn=<MinBackward1>) tensor(0.6859, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tdnn.tdnn import TDNN as TDNNLayer\n",
    "\n",
    "tdnn1 = TDNNLayer(\n",
    "  16, # input dim\n",
    "  8, # output dim\n",
    "  [-1,0,1], # context\n",
    ")\n",
    "\n",
    "tdnn2 = TDNNLayer(\n",
    "  8, # input dim\n",
    "  3, # output dim\n",
    "  [-2,0,2], # context\n",
    ")\n",
    "\n",
    "layer3 = nn.Conv1d(3, 50, 1)\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "input1 = []\n",
    "for i in range(50):\n",
    "    curr = []\n",
    "    for j in range(16):\n",
    "        currow = []\n",
    "        for j in range(15):\n",
    "            currow.append(random.uniform(0, 1))\n",
    "        curr.append(currow)\n",
    "    input1.append(curr)\n",
    "# input1 = np.array(input1)\n",
    "# input1 = np.zeros((16, 16, 16))\n",
    "input1 = torch.Tensor(input1)\n",
    "print(input1.shape)\n",
    "\n",
    "output1 = tdnn1(input1)\n",
    "print(output1.shape)\n",
    "output2 = tdnn2(output1)\n",
    "print(output2.shape)\n",
    "\n",
    "res = layer3(output2)\n",
    "res = sigmoid(res)\n",
    "print(res.shape)\n",
    "#flatten layer\n",
    "print(res.min (), res.max ())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8085d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer 1\n",
    "context = [0, 2]\n",
    "input_dim = 16\n",
    "output_dim = 8\n",
    "net = TDNN(context, input_dim, output_dim, full_context=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5996d974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 15, 16])\n",
      "torch.Size([50, 8, 13])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "input1 = []\n",
    "for i in range(50):\n",
    "    curr = []\n",
    "    for j in range(15):\n",
    "        currow = []\n",
    "        for j in range(16):\n",
    "            currow.append(random.uniform(0, 1))\n",
    "        curr.append(currow)\n",
    "    input1.append(curr)\n",
    "# input1 = np.array(input1)\n",
    "# input1 = np.zeros((16, 16, 16))\n",
    "input1 = torch.Tensor(input1)\n",
    "print(input1.shape)\n",
    "output = net(input1)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df111b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer 2\n",
    "context = [0,5]\n",
    "input_dim = 8\n",
    "output_dim = 3\n",
    "net = TDNN(context, input_dim, output_dim, full_context=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c18407c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [3, 8, 6], expected input[50, 13, 6] to have 8 channels, but got 13 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output2 \u001b[38;5;241m=\u001b[39m net(output)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(output2\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [1], line 39\u001b[0m, in \u001b[0;36mTDNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mx is one batch of data\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mx.size(): [batch_size, sequence_length, input_dim]\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03msequence length is the length of the input spectral data (number of frames) or if already passed through the convolutional network, it's the number of learned features\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03moutput size: [batch_size, output_dim, len(valid_steps)]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Check if parameters are cuda type and change context\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# if type(self.bias.data) == torch.cuda.FloatTensor and self.cuda_flag == False:\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#     self.context = self.context.cuda()\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#     self.cuda_flag = True\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m conv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecial_convolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(conv_out)\n",
      "Cell \u001b[0;32mIn [1], line 59\u001b[0m, in \u001b[0;36mTDNN.special_convolution\u001b[0;34m(self, x, kernel, context, bias)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(valid_steps):\n\u001b[1;32m     58\u001b[0m     features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mindex_select(x, \u001b[38;5;241m2\u001b[39m, context\u001b[38;5;241m+\u001b[39mi)\n\u001b[0;32m---> 59\u001b[0m     xs[:,:,c] \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m[:,:,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xs\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [3, 8, 6], expected input[50, 13, 6] to have 8 channels, but got 13 channels instead"
     ]
    }
   ],
   "source": [
    "output2 = net(output)\n",
    "print(output2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5fde621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "73c54537",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "res = m(output2)\n",
    "def sigmoid(Module):\n",
    "    def __init__(self, M):\n",
    "        # M is the dimension of input feature\n",
    "        super(network, self).__init__()\n",
    "        self.layer1 = nn.Linear(M, 3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return F.sigmoid(self.out(self.layer1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b14f946d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5000, 0.9683, 0.6393, 0.5000, 0.5000],\n",
      "         [0.5790, 0.5000, 0.9997, 0.5000, 0.7838],\n",
      "         [0.9749, 0.5000, 0.9076, 0.9925, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.9349, 0.7289, 0.5000, 0.5546],\n",
      "         [0.6372, 0.5000, 0.9644, 0.8038, 0.6664],\n",
      "         [0.9940, 0.5000, 0.7488, 0.9680, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.7320, 0.5000, 0.6013],\n",
      "         [0.7973, 0.9545, 0.6180, 0.5000, 0.5970],\n",
      "         [0.8972, 0.5000, 0.6526, 0.9733, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.8734, 0.7013, 0.5000, 0.9024],\n",
      "         [0.6133, 0.5000, 0.9815, 0.5269, 0.6266],\n",
      "         [0.9104, 0.5000, 0.8537, 0.9972, 0.7114]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "         [0.5931, 0.5000, 0.9670, 0.5000, 0.6617],\n",
      "         [0.9873, 0.5000, 0.5867, 0.9951, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.7412, 0.5000, 0.5000],\n",
      "         [0.6234, 0.5000, 0.8017, 0.5000, 0.5339],\n",
      "         [0.5178, 0.5000, 0.5000, 0.9408, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.5569, 0.5940, 0.5000, 0.5000],\n",
      "         [0.7058, 0.5000, 0.8282, 0.6084, 0.5457],\n",
      "         [0.8447, 0.5000, 0.6725, 0.9509, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.7701, 0.5000, 0.5000],\n",
      "         [0.7462, 0.5000, 0.8706, 0.7751, 0.5229],\n",
      "         [0.9957, 0.6565, 0.8136, 0.9489, 0.5000]],\n",
      "\n",
      "        [[0.5460, 0.5000, 0.7776, 0.5571, 0.5000],\n",
      "         [0.9755, 0.5000, 0.9458, 0.5000, 0.7571],\n",
      "         [0.9503, 0.6411, 0.7077, 0.9949, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.6735, 0.7859, 0.5000, 0.8654],\n",
      "         [0.9636, 0.5000, 0.8499, 0.6057, 0.9106],\n",
      "         [0.9786, 0.5000, 0.5000, 0.9970, 0.7605]],\n",
      "\n",
      "        [[0.5000, 0.8599, 0.5580, 0.5000, 0.5000],\n",
      "         [0.6373, 0.6115, 0.9890, 0.5000, 0.6085],\n",
      "         [0.6766, 0.5000, 0.6162, 0.9972, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.5929, 0.5000, 0.5000],\n",
      "         [0.8249, 0.7582, 0.5000, 0.5000, 0.6666],\n",
      "         [0.9182, 0.5000, 0.6680, 0.9881, 0.5145]],\n",
      "\n",
      "        [[0.5000, 0.5461, 0.8648, 0.5000, 0.5000],\n",
      "         [0.9563, 0.6438, 0.9783, 0.7406, 0.5000],\n",
      "         [0.9971, 0.5000, 0.8199, 0.9805, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.6699, 0.5000, 0.5187],\n",
      "         [0.6599, 0.7711, 0.8489, 0.5000, 0.8715],\n",
      "         [0.9889, 0.5000, 0.5911, 0.9845, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.7987, 0.5000, 0.5000],\n",
      "         [0.9731, 0.5000, 0.9875, 0.5000, 0.6162],\n",
      "         [0.9296, 0.5000, 0.9204, 0.9940, 0.5000]],\n",
      "\n",
      "        [[0.6512, 0.5000, 0.6945, 0.5000, 0.8078],\n",
      "         [0.5000, 0.6387, 0.9892, 0.5000, 0.7444],\n",
      "         [0.9259, 0.5000, 0.9275, 0.9872, 0.7222]]],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444d72c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
